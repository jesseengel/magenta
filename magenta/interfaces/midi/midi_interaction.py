"""A module for implementing interaction between MIDI and SequenceGenerators."""

import abc
import threading
import time

#internal imports
import tensorflow as tf

from magenta.protobuf import generator_pb2
from magenta.protobuf import music_pb2


class MidiInteractionException(Exception):
  """Base class for exceptions in this module."""
  pass


# TODO(adarob): Move to sequence_utils.
def merge_sequence_notes(sequence_1, sequence_2):
  """Returns a new NoteSequence combining the notes from both inputs.

  All fields aside from `notes` and `total_time` are copied from the first
  input.

  Args:
    sequence_1: A NoteSequence to merge. All fields aside from `notes` and
        `total_time` are copied directly from this sequence in the merged
        sequence.
    sequence_2: A NoteSequence to merge.

  Returns:
    A new NoteSequence combining the notes from the input sequences.
  """
  merged_sequence = music_pb2.NoteSequence(sequence_1)
  merged_sequence.notes.extend(sequence_2.notes)
  merged_sequence.total_time = max(sequence_1.end_time, sequence_2.end_time)


# TODO(adarob): Move to sequence_utils.
def filter_instrument(sequence, instrument, from_time=0):
  """Returns a new NoteSequence with notes from the given instrument removed.

  Only notes that start on or after `from_time` will be completely removed.
  Those that start before and end after `from_time` will be truncated to end
  at `from_time`.

  Args:
    sequence: The NoteSequence to created the filtered sequence from.
    instrument: The instrument number to remove notes of.
    from_time: The time on or after which to remove or truncate notes.

  Returns:
    A new NoteSequence with notes from the given instrument removed or truncated
    after `from_time`.
  """
  filtered_sequence = music_pb2.NoteSequence(sequence)
  del filtered_sequence.notes[:]
  for note in sequence.notes:
    if note.instrument == instrument:
      if note.start_time >= from_time:
        continue
      if note.end_time >= from_time:
        note.end_time = from_time
    filtered_sequence.notes.add().CopyFrom(note)
  return filtered_sequence


class MidiInteraction(threading.Thread):
  """Base class for handling interaction between MIDI and SequenceGenerator.

  Child classes will provided the "main loop" of an interactive session between
  a MidiHub used for MIDI I/O and sequences generated by a SequenceGenerator.

  Args:
    midi_hub: The MidiHub to use for MIDI I/O.
    qpm: The quarters per minute to use for this interaction.
  """
  _metaclass__ = abc.ABCMeta

  def __init__(self, midi_hub, qpm):
    self._midi_hub = midi_hub
    self._qpm = qpm
    # A signal to tell the main loop when to stop.
    self._should_stop = False
    super(MidiInteraction, self).__init__()

  def stop(self):
    """Stops the main loop, and blocks until the thread is terminated.

    Raises:
      MidiInteractionException: When the thread is not alive.
    """
    if not self.is_alive:
      raise MidiInteractionException(
          'Attempted to stop MidiInteraction that is not running.')
    self._should_stop = True
    self.join()


class AccompanimentMidiInteraction(MidiInteraction):
  """Implementation of a MidiInteraction for generating real-time accompaniment.

  Input from the MidiHub is continuously captured and passed to a
  SequenceGenerator to predict what an accompanying voice should play in the
  near future. This generated accompaniment is then played by the MidiHub.

  Args:
    midi_hub: The MidiHub to use for MIDI I/O.
    qpm: The quarters per minute to use for this interaction.
    sequence_generator: The SequenceGenerator to use to generate the
        accompanying voice in this interaction.
    predictahead_quarters: The number of quarter notes to start prediction past
        the end of the captured sequence. Should be determined by how the model
        underlying the generator was trained.
  """

  def __init__(self, midi_hub, qpm, sequence_generator, predictahead_quarters):
    super(AccompanimentMidiInteraction, self).__init__(midi_hub, qpm)
    self._sequence_generator = sequence_generator
    self._predicathead_quarters = predictahead_quarters

  @abc.abstractmethod
  def run(self):
    """The main loop for a real-time accompaniment interaction.

    Continuously captures input from the MidiHub while repeatedly generating
    additional steps of the accompaniment sequence and playing it back via
    the MidiHub. Stops when `_should_stop` is set to true (by the `stop`
    method).
    """
    # How should we handle the start time? Wait until the first note is played?
    quarter_duration = 60.0 / self._qpm
    start_quarters = (time.time() + 1.0) // quarter_duration

    # Offset of end of accompaniment in quarter notes from the epoch.
    accompaniment_quarters = start_quarters
    accompaniment_sequence = music_pb2.NoteSequence()

    # Start metronome.
    self._midi_hub.start_metronome(start_quarters * quarter_duration, self._qpm)
    # Start captor.
    captor = self._midi_hub.start_capture(self._qpm,
                                          start_quarters * quarter_duration)
    # Start player.
    player = self._midi_hub.start_playback(
        accompaniment_sequence, stay_alive=True)
    while not self._should_stop:
      # Offset of end of captured sequence in quarter notes from the epoch.
      capture_end_quarters = time.time() // quarter_duration
      captured_sequence = captor.captured_sequence(
          end_time=capture_end_quarters * quarter_duration)
      generation_end_quarters = (
          capture_end_quarters + self._prediction_quarters)
      request = generator_pb2.GenerateSequenceRequest()
      request.input_sequence.CopyFrom(
          merge_sequence_notes(captured_sequence, accompaniment_sequence))
      section = request.generator_options.generate_sections.add(
          start_time_seconds=accompaniment_quarters * quarter_duration,
          end_time_seconds=generation_end_quarters * quarter_duration)

      # Generate additional accompaniment notes.
      response = self._sequence_generator.generate(request)
      accompaniment_sequence = filter_instrument(response.generated_sequence, 0)

      # Update player with extended accompaniment.
      player.upate_sequence(accompaniment_sequence)

      # Compute and log delta time between end of accompaniment before update
      # when the extension generation completed..
      delta_time = time.time() - (accompaniment_quarters * quarter_duration)
      if delta_time < 0:
        tf.logging.warn('Generator is lagging by %02f seconds.', -delta_time)
      else:
        tf.logging.debug('Generator is ahead by %02f seconds.', delta_time)

      accompaniment_quarters = generation_end_quarters

    # Stop metronome.
    self._midi_hub.stop_metronome()
    # Stop captor.
    captor.stop()
    # Stop player.
    player.stop()


class CallAndResponseMidiInteraction(MidiInteraction):
  """Implementation of a MidiInteraction for real-time "call and response".

  Alternates between receiving input from the MidiHub ("call" phase) and
  playing generated sequences ("response" phase). During the call phase, the
  input is captured and used to generate the response, which is then played back
  during the response phase.

  Args:
    midi_hub: The MidiHub to use for MIDI I/O.
    qpm: The quarters per minute to use for this interaction.
    sequence_generator: The SequenceGenerator to use to generate the responses
        in this interaction.
    quarters_per_bar: The number of quarter notes in each bar/measure.
    phase_bars: The optional number of bars in each phase. `end_call_signal`
        must be provided if None.
    end_call_signal: The optional mido.Message to use as a signal to stop the
        call phase at the end of the current bar. `phase_bars` must be provided
        if None.
  """

  def __init__(self,
               midi_hub,
               qpm,
               sequence_generator,
               quarters_per_bar=4,
               phase_bars=None,
               end_call_signal=None):
    super(AccompanimentMidiInteraction, self).__init__(midi_hub, qpm)
    self._sequence_generator = sequence_generator
    self._quarters_per_bar = quarters_per_bar
    self._phase_bars = phase_bars
    self._end_call_signal = end_call_signal

  @abc.abstractmethod
  def run(self):
    """The main loop for a real-time call and response interaction.

    Alternates between receiving input from the MidiHub ("call" phase) and
    playing generated sequences ("response" phase). During the call phase, the
    input is captured and used to generate the response, which is then played
    back during the response phase.
    """
    # How should we handle the start time? Wait until the first note is played?
    quarter_duration = 60.0 / self._qpm
    start_quarters = (time.time() + 1.0) // quarter_duration

    bar_duration = quarter_duration * self._quarters_per_bar
    # The number of notes before call phase ends to start generation for
    # response phase. Will be automatically adjusted to be as small as possible
    # while avoiding late response starts.
    predictahead_quarters = 1

    # Offset of end of accompaniment in quarter notes from the epoch.
    call_start_quarters = start_quarters

    # Start metronome.
    self._midi_hub.start_metronome(start_quarters * quarter_duration, self._qpm)
    while not self._should_stop:
      # Call phase.
      # Capture sequence.
      if self._phase_bars is not None:
        capture_quarters = (
            self._phase_bars * self._quarters_per_bar - predictahead_quarters)

        captured_sequence = self._midi_hub.capture_sequence(
            self._qpm,
            call_start_quarters * quarter_duration,
            stop_time=(
                (call_start_quarters + capture_quarters) * quarter_duration))
      else:
        captor = self._midi_hub.start_capture(call_start_quarters *
                                              quarter_duration, self._qpm)
        self._midi_hub.wait_for_signal(self._end_call_signal)
        remaining_call_quarters = (
            (time.time() // quarter_duration - start_quarters) %
            self._quarters_per_bar)
        if remaining_call_quarters < predictahead_quarters:
          remaining_call_quarters += self._quarters_per_bar
        captor.stop(stop_time=(
            (call_start_quarters + remaining_call_quarters) * quarter_duration))
        captured_sequence = captor.captured_sequence()
      # Generate sequence.
      capture_end_quarters = captured_sequence.total_time // quarter_duration
      response_start_quarters = capture_end_quarters + predictahead_quarters
      response_end_quarters = (response_start_quarters +
                               (capture_end_quarters - call_start_quarters))
      request = generator_pb2.GenerateSequenceRequest()
      request.input_sequence.CopyFrom(captured_sequence)
      section = request.generator_options.generate_sections.add(
          start_time_seconds=response_start_quarters * quarter_duration,
          end_time_seconds=response_end_quarters * quarter_duration)

      # Generate response.
      response = self._sequence_generator.generate(request)
      response_sequence = response.generated_sequence

      # Response phase.
      # Start response playback.
      self._midi_hub.start_playback(response_sequence)

      # Compute and log delta time between end of accompaniment before update
      # when the extension generation completed..
      delta_time = time.time() - (response_start_quarters * quarter_duration)
      if delta_time < 0:
        predictahead_quarters += 1
        tf.logging.info('Generator is lagging by %02f seconds. '
                        'Increasing predictahead_quarters to %d.', -delta_time,
                        predictahead_quarters)
      elif predictahead_quarters > 1:
        predictahead_quarters -= 1
        tf.logging.info('Generator is ahead by %02f seconds. '
                        'Decreasing predictahead_quarters to %d.', delta_time,
                        predictahead_quarters)

      call_start_quarters = response_end_quarters

    # Stop metronome.
    self._midi_hub.stop_metronome()
